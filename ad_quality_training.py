# -*- coding: utf-8 -*-
"""ad_quality_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nObxKpRB6wips9wqmObJbYrtpHLNC_z-

# Ad Quality Analyzer Training Notebook
This notebook demonstrates loading the clickbait dataset, training a baseline model, and evaluating performance.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

#loading datasets
cb = pd.read_csv('clickbait_data.csv')
cb = cb[['headline','clickbait']].rename(columns={'headline':'text'})
cb['label'] = cb['clickbait'].map({1:'spam', 0:'neutral'})
cb = cb[['text','label']]

off = pd.read_csv('labeled_data.csv')
# In this file: class==2 → neither; class==0 or 1 → offensive
off = off[['tweet','class']].rename(columns={'tweet':'text'})
off['label'] = off['class'].apply(lambda c: 'neutral' if c==2 else 'offensive')
off = off[['text','label']]

#combining both datasets
df = pd.concat([cb, off], ignore_index=True)
print(df['label'].value_counts())

#vectorizing using Tfidf
vec = TfidfVectorizer(max_features=5_000, ngram_range=(1,2))
X = vec.fit_transform(df['text'])
le = LabelEncoder()
y = le.fit_transform(df['label'])   # maps ['neutral','offensive','spam'] → [0,1,2]

#train/test split (stratify to keep class balance)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=y
)

#applying logistic regression
model = LogisticRegression(
    multi_class='multinomial', solver='lbfgs', max_iter=1_000
)
model.fit(X_train, y_train)

#evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=le.classes_))
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred))

#testing function
def predict_text(text):
    X_ = vec.transform([text])
    probs = model.predict_proba(X_)[0]
    for cls, p in zip(le.classes_, probs):
        print(f"{cls:10s}: {p:.2f}")
    print("→  Predicted:", le.inverse_transform(model.predict(X_))[0])

#try it out yourself :)
predict_text("You won’t believe this hack!!!")
predict_text("Our quarterly report is out—download it now")
predict_text("Shes such a pain in the ass!")
predict_text("Your package is delayed. Click here to reschedule.")

